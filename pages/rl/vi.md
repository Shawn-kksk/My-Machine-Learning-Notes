# ä»·å€¼è¿­ä»£ (Value Iteration)

---

## æ ¸å¿ƒæ€æƒ³ä¸è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ (Core Idea & Bellman Optimality Equation)

 ä»·å€¼è¿­ä»£æ˜¯ä¸€ç§ç”¨äºåœ¨å·²çŸ¥**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)** æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯»æ‰¾æœ€ä¼˜ä»·å€¼å‡½æ•° $V^*(s)$ å’Œæœ€ä¼˜ç­–ç•¥ $\pi^*(s)$ çš„ç®—æ³• ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸æ–­è¿­ä»£æ›´æ–°æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œæœ€ç»ˆä½¿å…¶æ”¶æ•›åˆ°æœ€ä¼˜å€¼ ã€‚

 Value Iteration is an algorithm used to find the optimal value function $V^*(s)$ and the optimal policy $\pi^*(s)$ when the model of the **Markov Decision Process (MDP)** is known.  Its core idea is to iteratively update the value of each state until it converges to the optimal value.

 ä»·å€¼è¿­ä»£çš„åŸºç¡€æ˜¯**è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ (Bellman Optimality Equation)** ã€‚è¿™ä¸ªæ–¹ç¨‹è¡¨æ˜ï¼Œä¸€ä¸ªçŠ¶æ€çš„æœ€ä¼˜ä»·å€¼ $V^*(s)$ ç­‰äºæ™ºèƒ½ä½“åœ¨è¯¥çŠ¶æ€ä¸‹ï¼Œé€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€å¤§æœŸæœ›å›æŠ¥çš„è¡Œä¸ºåæ‰€èƒ½è·å¾—çš„ä»·å€¼ã€‚

 The foundation of value iteration is the **Bellman Optimality Equation**. This equation states that the optimal value of a state, $V^*(s)$, is equal to the value obtained by choosing the action that yields the maximum expected return in that state.

$$V^*(s) = \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*(s')]$$

---

## ç®—æ³•æ­¥éª¤ (Algorithm Steps)

ç®—æ³•çš„ä¼ªä»£ç å’Œæ­¥éª¤å¦‚ä¸‹ ï¼š
The pseudocode and steps for the algorithm are as follows:

1.  **åˆå§‹åŒ– (Initialization)**:
    * å¯¹æ‰€æœ‰çŠ¶æ€ $s$ï¼Œå°†ä»·å€¼å‡½æ•° $V_0(s)$ åˆå§‹åŒ–ä¸ºä»»æ„å€¼ï¼ˆé€šå¸¸ä¸º0ï¼‰ã€‚
    * For all states $s$, initialize the value function $V_0(s)$ to an arbitrary value (usually 0).

2.  **è¿­ä»£æ›´æ–° (Iterative Update)**:
    *  è¿›å…¥ä¸€ä¸ªå¾ªç¯ï¼Œå¯¹äºæ¯ä¸€ä¸ªçŠ¶æ€ $s \in S$ï¼Œæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹è¿›è¡Œæ›´æ–° ï¼š
    *  Enter a loop, and for each state $s \in S$, update according to the Bellman Optimality Equation:
    $$V_{t+1}(s) \leftarrow \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V_t(s')]$$

3.  **æ”¶æ•›åˆ¤æ–­ (Convergence Check)**:
    * å½“æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼å‡½æ•°å˜åŒ–é‡éƒ½å°äºä¸€ä¸ªé¢„è®¾çš„é˜ˆå€¼æ—¶ï¼Œå¾ªç¯åœæ­¢ã€‚
    * The loop stops when the change in the value function for all states is less than a predefined threshold.

---

## ä»ä»·å€¼å‡½æ•°ä¸­æå–ç­–ç•¥ (Extracting the Policy from the Value Function)

 å½“ä»·å€¼è¿­ä»£æ”¶æ•›åï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªè¿‘ä¼¼æœ€ä¼˜çš„ä»·å€¼å‡½æ•° $V^*(s)$ã€‚æˆ‘ä»¬éœ€è¦æœ€åä¸€æ­¥æ¥**æå–æœ€ä¼˜ç­–ç•¥ (extract a policy)** ã€‚å¯¹äºä»»ä½•çŠ¶æ€ $s$ï¼Œæœ€ä¼˜ç­–ç•¥ $\pi^*(s)$ å°±æ˜¯é€‰æ‹©é‚£ä¸ªèƒ½å¯¼å‡ºæœ€ä¼˜ä»·å€¼çš„è¡Œä¸º $a$ã€‚

After value iteration converges, we obtain an approximately optimal value function $V^*(s)$.  We need a final step to **extract the optimal policy**. For any state $s$, the optimal policy $\pi^*(s)$ is to choose the action $a$ that leads to this optimal value.

$$\pi^*(s) = \arg\max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*(s')]$$

---

## ä¼˜ç¼ºç‚¹åˆ†æ (Strengths and Weaknesses)

#### ä¼˜ç‚¹ (Strengths) ğŸ‘

*  **ä¿è¯æ”¶æ•› (Guaranteed Convergence)**: åªè¦æŠ˜æ‰£å› å­ $\gamma < 1$ï¼Œä»·å€¼è¿­ä»£ç®—æ³•ä¿è¯èƒ½å¤Ÿæ”¶æ•›åˆ°å”¯ä¸€çš„æœ€ä¼˜ä»·å€¼å‡½æ•° ã€‚
*  **Guaranteed Convergence**: As long as the discount factor $\gamma < 1$, the value iteration algorithm is guaranteed to converge to the unique optimal value function.

#### ç¼ºç‚¹ (Weaknesses) ğŸ‘

*  **è®¡ç®—æˆæœ¬é«˜ (High Computational Cost)**: æ¯ä¸€è½®è¿­ä»£éƒ½éœ€è¦éå†æ•´ä¸ªçŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œå¯¹äºçŠ¶æ€ç©ºé—´å¤§çš„é—®é¢˜ä¼šéå¸¸è€—æ—¶ ã€‚
*  **High Computational Cost**: Each iteration requires sweeping through the entire state and action spaces, which can be very time-consuming for problems with large state spaces.
*  **ä¾èµ–æ¨¡å‹ (Requires a Model)**: è¯¥ç®—æ³•è¦æ±‚MDPçš„æ‰€æœ‰å‚æ•°ï¼ˆè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°ï¼‰éƒ½æ˜¯å·²çŸ¥çš„ï¼Œè¿™åœ¨è®¸å¤šç°å®é—®é¢˜ä¸­éš¾ä»¥æ»¡è¶³ ã€‚
*  **Requires a Model**: The algorithm requires all parameters of the MDP (transition probabilities and reward function) to be known, which is often not feasible in many real-world problems.