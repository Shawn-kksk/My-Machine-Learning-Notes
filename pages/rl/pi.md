# ç­–ç•¥è¿­ä»£ (Policy Iteration)

---

## æ ¸å¿ƒæ€æƒ³ (Core Idea)

  ç­–ç•¥è¿­ä»£æ˜¯å¦ä¸€ç§åœ¨å·²çŸ¥**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)** æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ±‚è§£æœ€ä¼˜ç­–ç•¥çš„ç»å…¸ç®—æ³•  ã€‚ä¸ä»·å€¼è¿­ä»£ç›´æ¥è®¡ç®—æœ€ä¼˜ä»·å€¼å‡½æ•°ä¸åŒï¼Œç­–ç•¥è¿­ä»£åœ¨ä¸€ä¸ªå¾ªç¯ä¸­äº¤æ›¿è¿›è¡Œä¸¤ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š**ç­–ç•¥è¯„ä¼° (Policy Evaluation)** å’Œ **ç­–ç•¥æ”¹è¿› (Policy Improvement)** ã€‚å®ƒä»ä¸€ä¸ªä»»æ„çš„åˆå§‹ç­–ç•¥å¼€å§‹ï¼Œä¸æ–­åœ°è¯„ä¼°å¹¶æ”¹è¿›è¿™ä¸ªç­–ç•¥ï¼Œç›´åˆ°ç­–ç•¥ä¸å†å‘ç”Ÿå˜åŒ–ï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

  Policy Iteration is another classic algorithm for finding an optimal policy when the model of the **Markov Decision Process (MDP)** is known.   Unlike Value Iteration, which directly computes the optimal value function, Policy Iteration alternates between two core steps in a loop: **Policy Evaluation** and **Policy Improvement**. It starts with an arbitrary initial policy and iteratively evaluates and improves it until the policy no longer changes, thus finding the optimal policy.

---

## ç®—æ³•æ­¥éª¤ (Algorithm Steps)

  ç­–ç•¥è¿­ä»£ä¸»è¦åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªäº¤æ›¿è¿›è¡Œçš„æ­¥éª¤ ï¼š

  Policy Iteration mainly consists of the following two alternating steps:

### 1. ç­–ç•¥è¯„ä¼° (Policy Evaluation)

åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å›ºå®šå½“å‰çš„ç­–ç•¥ $\pi_t$ï¼Œç„¶åè®¡ç®—åœ¨è¯¥ç­–ç•¥ä¸‹æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼å‡½æ•° $V^{\pi_t}(s)$ã€‚è¿™ç›¸å½“äºè§£ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ç»„ï¼Œå…¶æ–¹ç¨‹æ˜¯é’ˆå¯¹å½“å‰ç­–ç•¥ $\pi_t$ çš„è´å°”æ›¼æ–¹ç¨‹ã€‚

In this step, we fix the current policy $\pi_t$ and then compute the value function $V^{\pi_t}(s)$ for each state under this policy. This is equivalent to solving a system of linear equations, where the equations are the Bellman equations for the current policy $\pi_t$.

$$V^{\pi_t}(s) = \sum_{s'} T(s, \pi_t(s), s') [R(s, \pi_t(s), s') + \gamma V^{\pi_t}(s')]$$

* è¿™é‡Œçš„ $\pi_t(s)$ æ˜¯å½“å‰ç­–ç•¥åœ¨çŠ¶æ€ $s$ ä¸‹æŒ‡å®šçš„è¡Œä¸ºã€‚
* Here, $\pi_t(s)$ is the action specified by the current policy in state $s$.

å®é™…ä¸Šï¼Œè¿™ä¸ªæ–¹ç¨‹ç»„å¯ä»¥é€šè¿‡è¿­ä»£çš„æ–¹å¼æ±‚è§£ï¼Œç±»ä¼¼äºä»·å€¼è¿­ä»£ï¼Œä½†å…³é”®åŒºåˆ«æ˜¯è¿™é‡Œçš„è¡Œä¸ºæ˜¯å›ºå®šçš„ï¼ˆç”±ç­–ç•¥ $\pi_t$ å†³å®šï¼‰ï¼Œè€Œä¸æ˜¯å–æœ€å¤§å€¼ã€‚

In practice, this system of equations can be solved iteratively, similar to value iteration, but the key difference is that the action is fixed (determined by policy $\pi_t$) instead of taking the maximum over all actions.

### 2. ç­–ç•¥æ”¹è¿› (Policy Improvement)

åœ¨è®¡ç®—å‡ºå½“å‰ç­–ç•¥ $\pi_t$ å¯¹åº”çš„ä»·å€¼å‡½æ•° $V^{\pi_t}$ åï¼Œæˆ‘ä»¬è¿›å…¥ç­–ç•¥æ”¹è¿›é˜¶æ®µã€‚å¯¹äºæ¯ä¸ªçŠ¶æ€ $s$ï¼Œæˆ‘ä»¬æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¯”å½“å‰ç­–ç•¥ $\pi_t(s)$ æ›´å¥½çš„è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡â€œå‘å‰çœ‹â€ä¸€æ­¥æ¥è´ªå©ªåœ°æ›´æ–°ç­–ç•¥ï¼š

After calculating the value function $V^{\pi_t}$ corresponding to the current policy $\pi_t$, we enter the policy improvement phase. For each state $s$, we check if there is a better action than the one specified by the current policy $\pi_t(s)$. We greedily update the policy by looking one step ahead:

$$\pi_{t+1}(s) \leftarrow \arg\max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V^{\pi_t}(s')]$$

* å¦‚æœæ–°çš„ç­–ç•¥ $\pi_{t+1}$ ä¸æ—§çš„ç­–ç•¥ $\pi_t$ å®Œå…¨ç›¸åŒï¼Œé‚£ä¹ˆç®—æ³•æ”¶æ•›ï¼Œæˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†æœ€ä¼˜ç­–ç•¥ $\pi^*$ å’Œæœ€ä¼˜ä»·å€¼å‡½æ•° $V^*$ã€‚
* If the new policy $\pi_{t+1}$ is identical to the old policy $\pi_t$, then the algorithm has converged, and we have found the optimal policy $\pi^*$ and the optimal value function $V^*$.
* å¦åˆ™ï¼Œæˆ‘ä»¬å°† $\pi_{t+1}$ ä½œä¸ºæ–°çš„å½“å‰ç­–ç•¥ï¼Œå¹¶è¿”å›åˆ°æ­¥éª¤1ï¼ˆç­–ç•¥è¯„ä¼°ï¼‰ã€‚
* Otherwise, we set $\pi_{t+1}$ as the new current policy and go back to step 1 (Policy Evaluation).

---

## ä¸ä»·å€¼è¿­ä»£çš„æ¯”è¾ƒ (Comparison with Value Iteration)

* **ä»·å€¼è¿­ä»£**: åœ¨æ¯æ¬¡ä¸»å¾ªç¯ä¸­ï¼Œåªå¯¹ä»·å€¼å‡½æ•°è¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼Œç„¶åéšå¼åœ°è°ƒæ•´ç­–ç•¥ã€‚å®ƒå°†è¯„ä¼°å’Œæ”¹è¿›åˆå¹¶åœ¨ä¸€ä¸ªæ›´æ–°è§„åˆ™ä¸­ã€‚
* **Value Iteration**: In each main loop, it performs only one update on the value function and then implicitly adjusts the policy. It combines evaluation and improvement into a single update rule.

* **ç­–ç•¥è¿­ä»£**: åœ¨æ¯æ¬¡ä¸»å¾ªç¯ä¸­ï¼Œå®ƒä¼šå®Œæ•´åœ°è¿›è¡Œç­–ç•¥è¯„ä¼°ï¼ˆå¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½æ”¶æ•›å¾—åˆ° $V^{\pi_t}$ï¼‰ï¼Œç„¶åå†è¿›è¡Œä¸€æ¬¡å®Œæ•´çš„ç­–ç•¥æ”¹è¿›ã€‚
* **Policy Iteration**: In each main loop, it performs a full policy evaluation (which may require multiple iterations to converge to $V^{\pi_t}$), followed by a complete policy improvement.

---

## ä¼˜ç¼ºç‚¹åˆ†æ (Strengths and Weaknesses)

#### ä¼˜ç‚¹ (Strengths) ğŸ‘

* **æ”¶æ•›é€Ÿåº¦å¿« (Fast Convergence)**: å°½ç®¡æ¯æ¬¡è¿­ä»£çš„è®¡ç®—é‡å¯èƒ½å¾ˆå¤§ï¼ˆå°¤å…¶æ˜¯åœ¨ç­–ç•¥è¯„ä¼°é˜¶æ®µï¼‰ï¼Œä½†ç­–ç•¥è¿­ä»£é€šå¸¸åªéœ€è¦å¾ˆå°‘çš„è¿­ä»£æ¬¡æ•°å°±èƒ½æ”¶æ•›ã€‚å¯¹äºè®¸å¤šé—®é¢˜ï¼Œå®ƒæ¯”ä»·å€¼è¿­ä»£æ”¶æ•›å¾—æ›´å¿«ã€‚
* **Fast Convergence**: Although each iteration can be computationally intensive (especially the policy evaluation step), policy iteration often converges in a very small number of iterations. For many problems, it converges faster than value iteration.

#### ç¼ºç‚¹ (Weaknesses) ğŸ‘

* **å•æ¬¡è¿­ä»£æˆæœ¬é«˜ (High Cost per Iteration)**: ç­–ç•¥è¯„ä¼°æ­¥éª¤éœ€è¦æ±‚è§£ä¸€ä¸ªå®Œæ•´çš„çº¿æ€§æ–¹ç¨‹ç»„æˆ–è¿›è¡Œå¤šæ¬¡è¿­ä»£æ‰«æï¼Œè¿™å¯èƒ½éå¸¸è€—æ—¶ï¼Œç‰¹åˆ«æ˜¯å½“çŠ¶æ€ç©ºé—´å¾ˆå¤§æ—¶ã€‚
* **High Cost per Iteration**: The policy evaluation step requires solving a full system of linear equations or performing multiple iterative sweeps, which can be very time-consuming, especially when the state space is large.
* **ä¾èµ–æ¨¡å‹ (Requires a Model)**: å’Œä»·å€¼è¿­ä»£ä¸€æ ·ï¼Œç­–ç•¥è¿­ä»£ä¹Ÿéœ€è¦çŸ¥é“ç¯å¢ƒçš„å®Œæ•´æ¨¡å‹ï¼ˆè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°ï¼‰ã€‚
* **Requires a Model**: Like value iteration, policy iteration also requires complete knowledge of the environment's model (transition probabilities and reward function).